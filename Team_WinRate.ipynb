{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f9394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define your models\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as imPipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a011bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean data\n",
    "df = pd.read_csv(\"players_data.csv\", encoding='latin1')\n",
    "df = df.drop_duplicates()\n",
    "df.dropna(subset=['character', 'gameId', 'gameRank'], inplace=True)\n",
    "df.fillna('Unknown', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1463a155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create team ID\n",
    "df['teamId'] = df['gameId'].astype(str) + \"_\" + df['gameRank'].astype(str)\n",
    "df['win'] = df['gameRank'].apply(lambda x: 1 if x <= 3 else 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9920c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zai\\AppData\\Local\\Temp\\ipykernel_19620\\3937751503.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  team_df = df.groupby('teamId').apply(lambda g: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "# Aggregate team composition and compute team stats\n",
    "team_df = df.groupby('teamId').apply(lambda g: pd.Series({\n",
    "    'gameId': g['gameId'].iloc[0],\n",
    "    'gameRank': g['gameRank'].iloc[0],\n",
    "    'win': g['win'].iloc[0],\n",
    "    'characters': sorted(list(g['character'])),\n",
    "    'kills': g['Kill'].sum(),\n",
    "    'deaths': g['Death'].sum(),\n",
    "    'assists': g['Assist'].sum(),\n",
    "    'damage': g['Dmg_Player'].sum()\n",
    "})).reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d6dd704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only full teams\n",
    "team_df = team_df[team_df['characters'].apply(len) == 3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0308692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split characters into separate columns\n",
    "team_df[['character1', 'character2', 'character3']] = pd.DataFrame(team_df['characters'].to_list(), index=team_df.index)\n",
    "team_df.drop(columns=['characters', 'teamId', 'gameId', 'gameRank'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c054e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   win  kills  deaths  assists  damage      character1 character2 character3\n",
      "0    1     13      11       21   62112           Adina     Lenore  Li Dailin\n",
      "1    1      7      12       10   38223           Adela     Alonso      Yumin\n",
      "2    1     11      10       13   46516           Cathy      Katja       Yuki\n",
      "3    0      9       5       15   38821          Alonso     Celine       Hart\n",
      "4    0      8      10       12   57173            Emma    Estelle      Katja\n",
      "5    0      9       3       11   35192  Debi_&_Marlene      Fiora     Nadine\n",
      "6    0      5       4        6   20419         Hyunwoo      Katja       Yuki\n",
      "7    0      5       7        4   20290           Darko       Hart    Hyunwoo\n",
      "8    1     21       0       35   89062         Abigail     Magnus    Shoichi\n",
      "9    1      7      10       10   51343           Adina    Bernice    Hyunwoo\n"
     ]
    }
   ],
   "source": [
    "print(team_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e02b7a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = team_df[['character1', 'character2', 'character3', 'kills', 'deaths', 'assists', 'damage']]\n",
    "y = team_df['win']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e2cddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce3ea3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "categorical_features = ['character1', 'character2', 'character3']\n",
    "numerical_features = ['kills', 'deaths', 'assists', 'damage']\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "], remainder='passthrough')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12d6eb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83      6820\n",
      "           1       0.78      0.67      0.72      4505\n",
      "\n",
      "    accuracy                           0.79     11325\n",
      "   macro avg       0.79      0.77      0.78     11325\n",
      "weighted avg       0.79      0.79      0.79     11325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model pipeline (RFC)\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))\n",
    "])\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8307594a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Random Forest...\n",
      "Evaluating Gradient Boosting...\n",
      "Evaluating Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Zai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Zai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Zai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Zai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Decision Tree...\n",
      "Evaluating SVC...\n",
      "Evaluating KNN...\n",
      "Evaluating AdaBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Zai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Zai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Zai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Zai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Extra Trees...\n",
      "Evaluating XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [18:06:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Zai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [18:06:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Zai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [18:06:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Zai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [18:06:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Zai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [18:06:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 5716, number of negative: 8669\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 751\n",
      "[LightGBM] [Info] Number of data points in the train set: 14385, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.397358 -> initscore=-0.416484\n",
      "[LightGBM] [Info] Start training from score -0.416484\n",
      "[LightGBM] [Info] Number of positive: 5716, number of negative: 8669\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000363 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 751\n",
      "[LightGBM] [Info] Number of data points in the train set: 14385, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.397358 -> initscore=-0.416484\n",
      "[LightGBM] [Info] Start training from score -0.416484\n",
      "[LightGBM] [Info] Number of positive: 5716, number of negative: 8670\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 748\n",
      "[LightGBM] [Info] Number of data points in the train set: 14386, number of used features: 198\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.397331 -> initscore=-0.416600\n",
      "[LightGBM] [Info] Start training from score -0.416600\n",
      "[LightGBM] [Info] Number of positive: 5716, number of negative: 8670\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000354 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 749\n",
      "[LightGBM] [Info] Number of data points in the train set: 14386, number of used features: 198\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.397331 -> initscore=-0.416600\n",
      "[LightGBM] [Info] Start training from score -0.416600\n",
      "[LightGBM] [Info] Number of positive: 5716, number of negative: 8670\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 748\n",
      "[LightGBM] [Info] Number of data points in the train set: 14386, number of used features: 198\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.397331 -> initscore=-0.416600\n",
      "[LightGBM] [Info] Start training from score -0.416600\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "    \"SVC\": SVC(probability=True),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"Extra Trees\": ExtraTreesClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
    "    \n",
    "    scores = cross_validate(\n",
    "        pipeline,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n",
    "        return_train_score=False\n",
    "    )\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': (scores['test_accuracy'].mean(), scores['test_accuracy'].std()),\n",
    "        'precision': (scores['test_precision_macro'].mean(), scores['test_precision_macro'].std()),\n",
    "        'recall': (scores['test_recall_macro'].mean(), scores['test_recall_macro'].std()),\n",
    "        'f1': (scores['test_f1_macro'].mean(), scores['test_f1_macro'].std())\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results).T.sort_values(by='f1', ascending=False)\n",
    "\n",
    "# Split means and standard deviations into separate columns\n",
    "results_df[['accuracy_mean', 'accuracy_std']] = pd.DataFrame(results_df['accuracy'].to_list(), index=results_df.index)\n",
    "results_df[['precision_mean', 'precision_std']] = pd.DataFrame(results_df['precision'].to_list(), index=results_df.index)\n",
    "results_df[['recall_mean', 'recall_std']] = pd.DataFrame(results_df['recall'].to_list(), index=results_df.index)\n",
    "results_df[['f1_mean', 'f1_std']] = pd.DataFrame(results_df['f1'].to_list(), index=results_df.index)\n",
    "\n",
    "# Drop the original columns containing tuples\n",
    "results_df = results_df.drop(columns=['accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "96f13cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     accuracy_mean  accuracy_std  precision_mean  \\\n",
      "Gradient Boosting         0.793572      0.008490        0.788396   \n",
      "AdaBoost                  0.790124      0.006499        0.782204   \n",
      "LightGBM                  0.790347      0.010325        0.784655   \n",
      "Random Forest             0.788567      0.009367        0.782566   \n",
      "Logistic Regression       0.786788      0.011132        0.782587   \n",
      "XGBoost                   0.784396      0.009270        0.777825   \n",
      "Extra Trees               0.777723      0.007279        0.772027   \n",
      "SVC                       0.775053      0.006644        0.768527   \n",
      "KNN                       0.749694      0.003254        0.739172   \n",
      "Decision Tree             0.712602      0.007149        0.700693   \n",
      "\n",
      "                     precision_std  recall_mean  recall_std   f1_mean  \\\n",
      "Gradient Boosting         0.009390     0.774950    0.008968  0.779821   \n",
      "AdaBoost                  0.007489     0.776167    0.005450  0.778688   \n",
      "LightGBM                  0.011462     0.771917    0.010596  0.776574   \n",
      "Random Forest             0.010128     0.770202    0.010096  0.774747   \n",
      "Logistic Regression       0.012327     0.765531    0.011739  0.771232   \n",
      "XGBoost                   0.010228     0.766193    0.009468  0.770514   \n",
      "Extra Trees               0.008177     0.756771    0.007440  0.761949   \n",
      "SVC                       0.007152     0.754889    0.007422  0.759623   \n",
      "KNN                       0.003333     0.732849    0.004202  0.735376   \n",
      "Decision Tree             0.007080     0.702648    0.006416  0.701508   \n",
      "\n",
      "                       f1_std  \n",
      "Gradient Boosting    0.009083  \n",
      "AdaBoost             0.006160  \n",
      "LightGBM             0.010872  \n",
      "Random Forest        0.010156  \n",
      "Logistic Regression  0.012003  \n",
      "XGBoost              0.009732  \n",
      "Extra Trees          0.007690  \n",
      "SVC                  0.007356  \n",
      "KNN                  0.003913  \n",
      "Decision Tree        0.006819  \n"
     ]
    }
   ],
   "source": [
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce4a721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict_win_rate(char1, char2, char3, kills=0, deaths=0, assists=0, damage=0):\n",
    "    chars = sorted([char1, char2, char3])\n",
    "    input_df = pd.DataFrame([{\n",
    "        'character1': chars[0],\n",
    "        'character2': chars[1],\n",
    "        'character3': chars[2],\n",
    "        'kills': kills,\n",
    "        'deaths': deaths,\n",
    "        'assists': assists,\n",
    "        'damage': damage\n",
    "    }])\n",
    "    prob = clf.predict_proba(input_df)[0][1]\n",
    "    return f\"Estimated Win Probability (Top 3): {prob:.2%}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b8a6f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Win Probability (Top 3): 45.00%\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "print(predict_win_rate(\"Adina\", \"Lenore\", \"Li Dailin\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
